# TextExtractorParams

Parameters for the text extractor.  The text extractor generates dense vector embeddings optimized for semantic similarity search. It uses the E5-Large multilingual model to convert text into 1024-dimensional vectors that capture semantic meaning, enabling accurate retrieval of conceptually related documents.  **Text Chunking & Decomposition**:     The extractor supports splitting long texts into multiple documents using various strategies.     This is essential for RAG applications where you need granular retrieval of specific passages.      - split_by: Strategy for splitting text (characters, words, sentences, paragraphs, pages, none)     - chunk_size: Target size for each chunk (interpretation depends on split_by strategy)     - chunk_overlap: Number of units to overlap between chunks (helps preserve context)      **Strategy Guide**:     - CHARACTERS: Fastest, uniform sizes, may break words. Good for quick testing.     - WORDS: Better readability, may break sentences. Good for general text.     - SENTENCES: Preserves semantic units. Best for Q&A and precise retrieval.     - PARAGRAPHS: Natural document structure. Best for articles and documentation.     - PAGES: Explicit page boundaries. Best for PDFs and paginated content.     - NONE: No splitting. Use for short texts (<400 words).  **LLM Structured Extraction (NEW)**:     The extractor now supports LLM-powered structured extraction using `response_shape`.     Instead of just generating embeddings, you can extract custom structured data from text.      - response_shape: Natural language prompt OR explicit JSON schema     - llm_provider: LLM provider to use (openai, google, anthropic)     - llm_model: Specific model for extraction      **When to Use**:     - Extract entities, relationships, sentiment from text     - Generate structured summaries with custom fields     - Classify text into custom taxonomies     - Extract domain-specific metadata  **When to Use**:     - General semantic search (documents, articles, knowledge bases, FAQs)     - RAG (Retrieval Augmented Generation) applications requiring fast context retrieval     - Q&A systems matching questions to answers semantically     - Content recommendation based on similarity     - Chatbots finding relevant information from documentation     - Multi-language search (supports 100+ languages)     - Real-time search requiring low latency (<10ms per query)     - Structured extraction from text using LLMs (NEW)  **When NOT to Use**:     - Very short texts (1-5 words) where keyword matching might be sufficient     - When exact keyword/phrase matching is more important than semantic similarity  **Model Details**:     - Embedding Model: E5-Large (intfloat/multilingual-e5-large-instruct)     - Dimensions: 1024     - Context Length: 512 tokens (~400 words)     - Languages: 100+ (trained on multilingual data)     - Distance Metric: Cosine similarity     - Normalization: L2 normalized vectors  **Performance Characteristics**:     - Embedding Generation: 5ms per document (batched: 2ms/doc)     - Index Build: ~1 hour per 10M documents     - Query Time: 5-10ms for top-100 results     - Memory: ~4GB per 1M documents     - Scales linearly with document count  **Use Case Examples**:     1. **E-commerce Product Search**: Search 1M products by description, find semantically similar items     2. **Customer Support**: Match user questions to 10K FAQ articles with 85%+ accuracy     3. **Document RAG**: Retrieve relevant context chunks from 100K documents for LLM prompts     4. **News Article Discovery**: Find related articles across 1M news items     5. **Research Paper Search**: Semantic search over academic papers and abstracts     6. **Structured Extraction**: Extract entities, sentiment, topics from documents using LLMs (NEW)  **Limitations**:     - Cannot match exact phrases or technical terms reliably     - May miss documents that use different terminology for same concept     - Struggles with very domain-specific jargon or acronyms     - 512 token limit means long documents must be chunked     - Less effective for keyword-heavy queries (e.g., \"iPhone 15 Pro Max 256GB\")     - LLM extraction adds latency (only use when needed)  Requirements:     - text field: REQUIRED (string or text type)     - All chunking parameters are OPTIONAL     - LLM parameters are OPTIONAL (only for structured extraction)

## Properties

Name | Type | Description | Notes
------------ | ------------- | ------------- | -------------
**extractor_type** | **str** | Discriminator field for parameter type identification. Must be &#39;text_extractor&#39;. | [optional] [default to 'text_extractor']
**split_by** | [**TextSplitStrategy**](TextSplitStrategy.md) | OPTIONAL. Strategy for splitting text into multiple documents. Default is &#39;none&#39; (no splitting, entire text becomes one document). Options: &#39;characters&#39; - Split by character count (fastest, may break words). &#39;words&#39; - Split by word boundaries (preserves words). &#39;sentences&#39; - Split by sentence boundaries (preserves semantic units). &#39;paragraphs&#39; - Split by paragraph boundaries (best for articles). &#39;pages&#39; - Split by page breaks (best for paginated documents). &#39;none&#39; - No splitting (default). Choose based on your content structure and retrieval granularity needs. | [optional] 
**chunk_size** | **int** | OPTIONAL. Target size for each chunk. Interpretation depends on split_by strategy: - characters: Number of characters per chunk (e.g., 1000 chars). - words: Number of words per chunk (e.g., 200 words). - sentences: Number of sentences per chunk (e.g., 5 sentences). - paragraphs: Number of paragraphs per chunk (e.g., 2 paragraphs). - pages: Number of pages per chunk (e.g., 1 page). - none: Ignored (entire text processed as one document). Default: 1000. Recommended ranges: characters (500-2000), words (100-400), sentences (3-10). | [optional] [default to 1000]
**chunk_overlap** | **int** | OPTIONAL. Number of units to overlap between consecutive chunks. Helps preserve context across chunk boundaries. Units match split_by strategy (characters, words, sentences, etc.). Example: With chunk_size&#x3D;1000 and chunk_overlap&#x3D;100, chunks will be: [0-1000], [900-1900], [1800-2800], etc. Default: 0 (no overlap). Recommended: 10-20% of chunk_size (e.g., 100-200 for chunk_size&#x3D;1000). Higher overlap improves context but increases storage and processing time. | [optional] [default to 0]
**response_shape** | [**ResponseShape2**](ResponseShape2.md) |  | [optional] 
**llm_provider** | **str** | OPTIONAL. LLM provider to use for structured extraction. Only required if response_shape is provided. Supported providers: &#39;openai&#39;, &#39;google&#39;, &#39;anthropic&#39;. Default: &#39;openai&#39; if not specified. | [optional] 
**llm_model** | **str** | OPTIONAL. Specific LLM model for structured extraction. Only required if response_shape is provided. Examples: - OpenAI: &#39;gpt-4o-mini-2024-07-18&#39; (efficient), &#39;gpt-4o-2024-08-06&#39; (best quality) - Google: &#39;gemini-2.0-flash&#39; (fastest, recommended) - Anthropic: &#39;claude-3-5-haiku-20241022&#39; (fast), &#39;claude-3-5-sonnet-20241022&#39; (best reasoning) Default: Uses provider&#39;s recommended model if not specified. | [optional] 

## Example

```python
from mixpeek.models.text_extractor_params import TextExtractorParams

# TODO update the JSON string below
json = "{}"
# create an instance of TextExtractorParams from a JSON string
text_extractor_params_instance = TextExtractorParams.from_json(json)
# print the JSON string representation of the object
print(TextExtractorParams.to_json())

# convert the object into a dict
text_extractor_params_dict = text_extractor_params_instance.to_dict()
# create an instance of TextExtractorParams from a dict
text_extractor_params_from_dict = TextExtractorParams.from_dict(text_extractor_params_dict)
```
[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)


